#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
File    : getpaper
Author  : A. Dareau

Comments: a small command-line tool to get papers from their references. It
          uses the journals' search engine, automatically send to them a well
          formatted request, and scraps the answer (using BeautifulSoup) to get
          the paper url. If a paper is found, it opens its url on a new tab in
          the web browser. If there is an error, it opens the paper search page
          
          Example :
              $> getpaper PRA 46 2668
              $> getpaper Nature 519 211
              $> getpaper arxiv 1706 07781
"""
#%% Imports
import requests
import webbrowser
import sys

from bs4 import BeautifulSoup

#%% Global Variables
# paper reference initialization
JOURNAL = 'PRA'
ISSUE = '46'
PAGE = '2668'

# list of available journals
APS_LIST = ['PRL', 'PRX', 'RMP', 'PRA', 'PRB', 'PRC', 'PRD', 'PRE', 'PR']
SCIENCE_LIST = ['Science']
ARXIV_LIST = ['arxiv', 'arXiv']

NATURE_NAMES = {'Nature':'nature', 'NatPhys':'nphys'}
NATURE_LIST = NATURE_NAMES.keys()

IOP_NAMES = {'JPBold':'0022-3700', 'JPB':'0953-4075', 'NJP':'1367-2630'}
IOP_LIST = IOP_NAMES.keys()

OSA_NAMES = {'OL':'ol', # optics letters
             'OE':'oe', # optics express
             'Optica':'optica', 'AO':'ao'}
OSA_LIST = OSA_NAMES.keys()

ALL_JOURNALS = APS_LIST + SCIENCE_LIST + NATURE_LIST + ARXIV_LIST + IOP_LIST \
               + OSA_LIST

#%% Help string

HELP_STRING = """
          a small command-line tool to get papers from their references. It
          uses the journals' search engine, automatically send to them a well
          formatted request, and scraps the answer (using BeautifulSoup) to get
          the paper url. If a paper is found, it opens its url on a new tab in
          the web browser. If there is an error, it opens the paper search page
          
          Example :
              $> getpaper PRA 46 2668
              $> getpaper Nature 519 211
              $> getpaper arxiv 1706 07781 
              
          Other options :
              $> getpaper journals : returns implemented journal list
           """
              
#%% Core functions
def start():
    '''
    Will analyse input and act accordingly
    '''
    global ALL_JOURNALS, HELP_STRING
    if len(sys.argv)==1 or len(sys.argv) > 4:
        print(HELP_STRING)
        out = False
    elif len(sys.argv)==2:
        option = sys.argv[1]
        if option == 'journals':
            msg = 'available journals : '+ ', '.join(ALL_JOURNALS)
            print(msg)
        out = False
    else:
        out = True
        
    return out
    
    
def get_paper_properties():
    global JOURNAL, ISSUE, PAGE, ALL_JOURNALS
    # check number of inputs
    err_msg = 'getpaper takes exactly 3 arguments : journal, issue, page'
    err_msg += '\n' + 'example : getpaper PRA 46 2668'
    assert len(sys.argv) == 4, err_msg
    # get inputs
    JOURNAL, ISSUE, PAGE = sys.argv[1:]
    # check implemented journal
    err_msg = '%s is not an implemented journal'%JOURNAL
    err_msg += '\n' + 'available journals : ' + ', '.join(ALL_JOURNALS)
    assert JOURNAL in ALL_JOURNALS, err_msg
    
def send_search_request():
    global JOURNAL, ISSUE, PAGE, APS_LIST, SCIENCE_LIST, NATURE_LIST, \
           ARXIV_LIST, IOP_LIST, OSA_LIST
    # get reference
    if JOURNAL in APS_LIST:
        url = get_APS_reference()
    elif JOURNAL in SCIENCE_LIST:
        url = get_Science_reference()
    elif JOURNAL in NATURE_LIST:
        url = get_Nature_reference()
    elif JOURNAL in ARXIV_LIST:
        url = get_arxiv_reference()
    elif JOURNAL in IOP_LIST:
        url = get_IOP_reference()
    elif JOURNAL in OSA_LIST:
        url = get_OSA_reference() 
    else:
        url = 'https://scholar.google.com/'  
    # print url and go there
    print(' >>> going to %s'%url)
    webbrowser.open_new_tab(url)
        
#%% Paper retrieval functions
def get_APS_reference():
    '''
    The search server (https://journals.aps.org/search/citation) only takes 
    posts requests (does not return anything otherwise). It directly sends us
    to the paper's page.
    '''
    global JOURNAL, ISSUE, PAGE
    url = 'https://journals.aps.org/search/citation'
    data = {'journal':JOURNAL, 'volume':ISSUE, 'article':PAGE}
    r = requests.post(url, data = data)
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'lxml')
        url = soup.find("meta", property='og:url')
        if url != None:
            url = url['content']
        else:
            print('Error : paper not found')
            url = r.url
    else:
        print('Error : %i'%r.status_code)
        print(r.reason)

    return url

def get_Science_reference():
    '''
    for some strange reason, the search server (science.sciencemag.org/search/)
    does not receive its arguments via a get or a post request. So we have to
    concatenate the research 'by hand', in the form 'argument%3Avalue', whith
    white space as a separator.
    '''
    global ISSUE, PAGE
    url = 'http://science.sciencemag.org/search/'
    data = {'numresults':10, 'sort':'relevance-rank', 
            'format_result':'standard',
            'volume':ISSUE, 'firstpage':PAGE}
    params = [k + '%3A' + str(v) for k, v in data.iteritems() ]
    params = ' '.join(params)
    r = requests.get(url + params)
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'lxml')
        attrs = {'class':'highwire-cite-linked-title',
                 'data-hide-link-title':0}
        url = soup.find('a', attrs=attrs)
        if url != None:
            url = 'http://science.sciencemag.org' + url['href']
            pass
        else:
            print('Error : paper not found')
            url = r.url
    else:
        print('Error : %i'%r.status_code)
        print(r.reason)
        url = r.url
    
    return url

def get_Nature_reference():
    '''
    for Nature this is rather straightforward : the search server
    (https://www.nature.com/search) takes get requests. We scrap the
    first returned paper url looking for the 'search_result_rank_1' attribute.
    '''
    global JOURNAL, ISSUE, PAGE, NATURE_NAMES
    url = 'https://www.nature.com/search'
    data = {'journal':NATURE_NAMES[JOURNAL], 'order':'relevance', 
            'volume':ISSUE, 'spage':PAGE}
    r = requests.get(url, params = data)
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'lxml')
        attrs = {'itemprop':'url','data-track-source':'search_result_rank_1'}
        url = soup.find('a', attrs=attrs)
        if url != None:
            url = url['href']
            pass
        else:
            print('Error : paper not found')
            url = r.url
    else:
        print('Error : %i'%r.status_code)
        print(r.reason)
        url = r.url
    return url
    
def get_arxiv_reference():
    global JOURNAL, ISSUE, PAGE
    '''
    this one is actually quite easy... I just include it for completeness
    '''
    url = 'https://arxiv.org/abs/%s.%s'%(ISSUE, PAGE)
    return url
 

def get_IOP_reference():
    global JOURNAL, ISSUE, PAGE, IOP_NAMES
    '''
    for IOP papers we call the server (iopscience.iop.org/findcontent) with a
    simple get request. The journal name is coded (see list in source code of
    iopscience.iop.org/findcontent). A selection is implemented (in IOP_NAMES)
    '''
    url = 'http://iopscience.iop.org/findcontent'
    data = {'CF_JOURNAL':IOP_NAMES[JOURNAL], 'CF_VOLUME':ISSUE, 'CF_ISSUE':'',
            'CF_PAGE':PAGE, 'submit':'Go', 'navsubmit':'Go'}
    r = requests.get(url, params = data)
    url = r.url
    return r.url

def get_OSA_reference():
    '''
    for OSA papers we call the server (https://www.osapublishing.org/search.cfm)
    with a get request. Here the search page takes time to return a result, so
    we directly open it in the browser.. 
    #FIXME: find a way to wait for the result ?
    '''
    global JOURNAL, ISSUE, PAGE, OSA_NAMES
    url = 'https://www.osapublishing.org/search.cfm'
    data = {'j':OSA_NAMES[JOURNAL], 'q':'', 'i':'', 
            'v':ISSUE, 'p':PAGE}
            
    params = [k + '=' + str(v) for k, v in data.iteritems() ]
    params = '&'.join(params)
    url = url + '?' + params
    '''
    r = requests.get(url, params = data, allow_redirects=True,
                     stream=True, timeout=None)
    '''
    return url  
#%% Main
def main():
    if start():
        get_paper_properties()
        send_search_request()
    
#%% Execute
if __name__ == '__main__':
    main()
